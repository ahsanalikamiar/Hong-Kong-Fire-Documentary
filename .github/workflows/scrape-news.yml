# News Scraper Workflow
# Runs on self-hosted runner to scrape news articles from URLs in markdown files

name: Scrape News Articles

on:
  # Manual trigger with options
  workflow_dispatch:
    inputs:
      source:
        description: 'Specific source to scrape (e.g., BBC, HK01). Leave empty for all.'
        required: false
        default: ''
      limit:
        description: 'Maximum number of URLs to scrape'
        required: false
        default: ''
      dry_run:
        description: 'Dry run (show what would be scraped without scraping)'
        type: boolean
        default: false

  # Trigger on push to news markdown files (optional - uncomment to enable)
  # push:
  #   branches: [main]
  #   paths:
  #     - 'content/news/**/*.md'
  #     - 'content/news/**/*.MD'

  # Scheduled run (optional - uncomment and adjust cron as needed)
  # schedule:
  #   - cron: '0 6 * * *'  # Run daily at 6 AM UTC

jobs:
  scrape:
    name: Scrape News Articles
    # Use self-hosted runner - change to 'ubuntu-latest' for GitHub-hosted
    runs-on: self-hosted
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scripts/scraper/requirements.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scripts/scraper/requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Run scraper (dry run)
        if: ${{ github.event.inputs.dry_run == 'true' }}
        run: |
          cd scripts/scraper
          python scraper.py --dry-run \
            ${{ github.event.inputs.source && format('--source {0}', github.event.inputs.source) || '' }} \
            ${{ github.event.inputs.limit && format('--limit {0}', github.event.inputs.limit) || '' }} \
            --verbose

      - name: Run scraper
        if: ${{ github.event.inputs.dry_run != 'true' }}
        run: |
          cd scripts/scraper
          python scraper.py \
            ${{ github.event.inputs.source && format('--source {0}', github.event.inputs.source) || '' }} \
            ${{ github.event.inputs.limit && format('--limit {0}', github.event.inputs.limit) || '' }} \
            --verbose

      - name: Check for changes
        id: changes
        run: |
          git add -A
          if git diff --staged --quiet; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
          fi

      - name: Commit and push changes
        if: steps.changes.outputs.has_changes == 'true' && github.event.inputs.dry_run != 'true'
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          
          # Get count of new archives
          NEW_COUNT=$(git diff --staged --name-only | grep -c "archive/" || echo "0")
          
          git commit -m "chore: scrape ${NEW_COUNT} new news articles [skip ci]"
          git push

      - name: Summary
        run: |
          echo "## Scraper Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event.inputs.dry_run }}" == "true" ]; then
            echo "**Mode:** Dry run (no changes made)" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.changes.outputs.has_changes }}" == "true" ]; then
            echo "**Mode:** Live scrape" >> $GITHUB_STEP_SUMMARY
            echo "**Result:** Changes committed and pushed" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Mode:** Live scrape" >> $GITHUB_STEP_SUMMARY
            echo "**Result:** No new URLs to scrape" >> $GITHUB_STEP_SUMMARY
          fi

